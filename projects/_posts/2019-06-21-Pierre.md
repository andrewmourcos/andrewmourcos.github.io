---
layout: projects
title: Pierre — The AI Presentation Assistant
tag: AI, NLP, CV, Hackathon
description: Using AI to help people overcome public speaking anxiety.
tools: Python, OpenCV, PocketSphynx, STDlib
img: Media/Pierre-mockup.jpg
---
<img src="/Media/Pierre-mockup.jpg">
*Envisioned mockup for Pierre including speech analysis, motion detection and analytics pages*

I recently attended Enghack 2019 (a hackathon hosted by the University of Waterloo) with my fellow co-ops (<a href="https://ca.linkedin.com/in/yipeng-penny-ji">Penny Ji</a>, <a href="https://ca.linkedin.com/in/robert-toyonaga-421176149">Robert Toyonaga</a> and <a href="https://aidanphilpott.com/">Aidan Philpott</a>). A few hours before the hackathon, we were delivering our sprint demos at our office and were given some great tips on how to improve our presentations afterwards.

We realized the importance of practicing a presentation in front of a real audience — it's very difficult to catch your own mistakes. This is where our hackathon idea came to light. 

We came up with **Pierre — The AI Presentation Assistant** !

There are two main features to Pierre: 
1. Real-time speech analysis 
2. Body language detection.

## Speech Analysis
To enable Pierre to deliver feedback on the user's vocabulary, we used Natural Language Processing (NLP). Our system kept a bank of semantically redundant phrases (ex: you know, like + yeah, and + yeah, literally, etc) and listened to the presenter through their microphone. If the system detected these in the presentation, it would notify the presenter **immediately**. 

This is where Pierre differentiates itself from similar products — Pierre gives you **feedback while you present**. There is a lot of litterature around using real-time feedback for practicing presentations. It is common in communication workshops to get participants to deliver a speech while the audience members hold buzzers. If the speaker makes a mistake, the audience will alert them so they can adjust themselves.

Likewise, Pierre is connected to the user's smartphone and sends an SMS to alert them of poor vocabulary. SMS was chosen since it allows the user to place their phone in their pocket for discrete haptic feedback. Additionally, these messages are natively forwarded to the user's smart devices (smartwatches, etc).

## Visual Analysis
The second aspect of Pierre is it's ability to provide feedback on visual queues. Nervous presenters will often exhibit ticks such as swaying, tapping or pacing without even noticing! For Pierre's prototype, we developed some computer vision heuristics to determine if a presenter is moving too much with only a webcam.

<video controls src="/Media/pierre-vision.mp4" width="100%">
	Sorry, your browser doesn't support embedded videos.
</video>
*Demonstration of Pierre's visual analysis algorithm*

In the video above, we showed the live plot on the left which displays the horizontal and vertical velocity of the user's head. This is one of the features that we use to determine if the speaker is exhibiting ticks as it shows the cyclical nature. Along with other features, we collect this data from all keypoints (not just the person's head). Moreover, just like the speech analysis, Pierre will notify the user in real-time via SMS.

## What's next? 
We will be conducting more user research to help us with our product direction. The next big update will be a new GUI that allows users to view their live data as they are presenting and also compare their recent sessions. Additionally, we will be developing more robust tick/utterance detection algorithms.

<img src="/Media/pk-team.GIF">
*The Pierre team: Aidan Philpott, Penny Ji, myself and Robert Toyonaga*
